Chicago 311 Data Pipeline (DuckDB)

This project fetches Chicago 311 service request data from the City of Chicago open data API, ingests it into DuckDB, builds a star schema, computes features for modeling, and benchmarks machine learning models on service closure outcomes.

ğŸš€ Features

Incremental ingestion (2018â€‘12 â†’ 2025â€‘08) with monthly batching

DuckDB backend (columnar, compressed, analyticsâ€‘optimized)

Star schema ETL (dim_service, dim_agency, dim_location, dim_time, fact_requests)

Resumable ingestion via meta.ingestion_log

Feature engineering (closure time, categorical encodings, time splits)

Model benchmarking (LogReg, Random Forest, extensible to XGBoost)

Timeâ€‘based train/test splits

Configurable via conf/config.toml

Debug mode for small batch testing

ğŸ“‚ Project Structure
chicago-311-duckdb/
â”œâ”€ README.md                  
â”œâ”€ requirements.txt           
â”œâ”€ .env.example               
â”œâ”€ conf/
â”‚  â””â”€ config.toml             # API, DB, ETL, ingestion config
â”œâ”€ data/
â”‚  â”œâ”€ chicago_311.duckdb      # DuckDB database file
â”‚  â””â”€ logs/
â”‚     â””â”€ ingest.log           # Optional ingest log
â”œâ”€ notebooks/
â”‚  â””â”€ 01_eda.ipynb            # Jupyter notebook for exploration
â”œâ”€ results/
â”‚  â””â”€ benchmarks.csv          # Model benchmark results
â”œâ”€ src/
â”‚  â”œâ”€ db/duckdb_utils.py      # DuckDB helpers
â”‚  â”œâ”€ etl/                     # Star schema ETL scripts
â”‚  â”‚  â”œâ”€ 01_setup.py
â”‚  â”‚  â”œâ”€ 02_dimensions.py
â”‚  â”‚  â”œâ”€ 03_fact_prepare.py
â”‚  â”‚  â”œâ”€ 04_fact_insert.py
â”‚  â”‚  â””â”€ 05_validate.py
â”‚  â”œâ”€ ingest/ingest_monthly.py # Raw data ingestion
â”‚  â”œâ”€ features/build_features.py # Feature engineering
â”‚  â””â”€ model/benchmark.py      # Model benchmarking
â””â”€ Makefile                   # Convenience commands

âš™ï¸ Setup
1. Clone & Create Virtual Environment
git clone <repo_url>
cd chicago-311-duckdb
python3 -m venv chicago311_env        # create a unique venv
source chicago311_env/bin/activate    # activate the venv (zsh/macOS)


Windows: chicago311_env\Scripts\activate

Prompt will show (chicago311_env) when active.

2. Install Dependencies
pip install --upgrade pip
pip install -r requirements.txt

3. Configure
cp .env.example .env


Optional: Add CHI_APP_TOKEN (Socrata app token) for higher rate limits

Adjust conf/config.toml for batch sizes, DB path, ETL debug mode, and time ranges

ğŸ”„ ETL / Star Schema Usage

Run ETL scripts inside the activated virtual environment:

python src/etl/01_setup.py       # Drop & deduplicate raw requests
python src/etl/02_dimensions.py  # Create & populate dimension tables
python src/etl/03_fact_prepare.py # Prepare fact table with precomputed request_id
python src/etl/04_fact_insert.py # Insert fact records in batches
python src/etl/05_validate.py    # Sanity checks, counts, integrity


Use debug = true in conf/config.toml for small sample runs (~10 rows)

Production run (~12M rows) requires debug = false

Optional: Run all ETL scripts via Makefile

Add this target to your Makefile:

.PHONY: etl-run

etl-run:
	source chicago311_env/bin/activate && \
	pip install -r requirements.txt && \
	python src/etl/01_setup.py && \
	python src/etl/02_dimensions.py && \
	python src/etl/03_fact_prepare.py && \
	python src/etl/04_fact_insert.py && \
	python src/etl/05_validate.py


Run with:

make etl-run


Automatically activates venv, installs dependencies, and runs ETL in order

ğŸ”„ Data Ingestion
python -m src.ingest.ingest_monthly --start 2018-12 --end 2025-08


Downloads monthly 311 data

Stores into raw.requests table in DuckDB

Tracks progress in meta.ingestion_log

ğŸ›  Feature Engineering
python -m src.features.build_features --rebuild


Creates feat.requests table

Computes closure time, categorical features, target column

ğŸ‹ï¸ Model Benchmarking
python -m src.model.benchmark --train-start 2018-12 --train-end 2023-12 \
                              --test-start 2024-01 --test-end 2025-08


Benchmarks Logistic Regression & Random Forest

Outputs metrics to results/benchmarks.csv

Saves trained models to models/ (if folder exists)

ğŸ“Š Example Queries (DuckDB CLI / Python)
-- Total requests per month
SELECT _month_id, count(*) FROM raw.requests GROUP BY 1 ORDER BY 1;

-- Fact table join with service dimension
SELECT f.request_id, d.service_name
FROM fact_requests f
JOIN dim_service d ON f.service_id = d.service_id
LIMIT 10;

ğŸ§© Extending

Regression task (predict hours_to_close)

Add more models (XGBoost, LightGBM, neural nets)

Experiment tracking (MLflow / Weights & Biases)

Geospatial enrichment (wards, tracts)

âš ï¸ Notes

Raw dataset is multiâ€‘GB (~5GB+). DuckDB handles this efficiently

ETL is safe in debug mode; production mode can handle full dataset (~12M rows)

Feature script should be reâ€‘run after new ingestion

Validation script ensures integrity of star schema and helps debug fact insert issues

ğŸ“œ License

MIT (or your preferred license)